# DevOps Roadmap for Spring PetClinic Deployment

This document outlines the step-by-step process for taking the Spring PetClinic application from its current state to a fully automated, production-grade deployment using modern DevOps practices.

---

### **Phase 1: Foundational Setup & Continuous Integration (CI)**

*Goal: Solidify the build and test process, ensuring every code change is automatically validated.*

1.  **Formalize Branching Strategy**: Adopt a standard Git branching model like GitFlow. This typically involves `main` for production releases, `develop` for the latest integrated changes, and `feature/*` branches for new work.
2.  **Enhance CI Pipeline**: Review the existing GitHub Actions workflow (`maven-build.yml`).
    *   Ensure it triggers on all relevant branches (pull requests to `develop` and `main`).
    *   Add a step for static code analysis using a tool like **SonarQube** to catch bugs and security vulnerabilities early. (*Note: SonarQube integration is currently temporarily disabled to unblock the pipeline due to persistent configuration issues. It can be revisited later.*)
3.  **Secure a Build Tool**: Standardize on either Maven or Gradle for all CI/CD operations. The existing `./mvnw verify` is a perfect command for this phase as it compiles, runs tests, and packages the app.
4.  **(Optional) Publish JAR Artifact**: Configure the CI pipeline to publish the packaged `.jar` file to an artifact repository like **JFrog Artifactory** or **Sonatype Nexus**. This provides versioned, stable builds for rollbacks or manual distribution.

---

### **Phase 2: Containerization & Registry**

*Goal: Package the application into a portable container and store it in a secure, centralized registry.*

1.  **Automate Image Build**: Add a new step to the CI workflow that runs after successful testing. Use the built-in Spring Boot command to create an OCI-compliant image:
    ```bash
    ./mvnw spring-boot:build-image -Dspring-boot.build-image.imageName=your-registry/petclinic:${GITHUB_SHA}
    ```
2.  **Set Up Container Registry**: Provision a private container registry. **Amazon ECR (Elastic Container Registry)** is a common choice when deploying to AWS.
3.  **Automate Image Push**: Configure the CI workflow to authenticate with the container registry (using GitHub Secrets for credentials) and push the newly built image. This should only happen on successful merges to the `develop` or `main` branch.

---

### **Phase 3: Infrastructure as Code (IaC)**

*Goal: Define all necessary cloud infrastructure in code for repeatable, automated environment creation.*

**Status: âœ… Completed - Terraform code generated and ready for execution.**

**Tools & Technologies:**
*   **IaC Tool**: Terraform
*   **Cloud Provider**: AWS
*   **Services**: Amazon EKS (Elastic Kubernetes Service), Amazon RDS (Relational Database Service - PostgreSQL), Amazon VPC (Virtual Private Cloud), AWS IAM (Identity and Access Management)
*   **Region**: `us-east-1`

**Requirements for User:**
*   An active AWS Account.
*   AWS CLI installed and configured with credentials that have sufficient permissions to create VPCs, EKS clusters, RDS instances, and IAM roles.
*   Terraform CLI installed locally (version 1.0+ recommended).

**Process & Steps:**

1.  **Terraform Project Structure**:
    *   We will create a dedicated `terraform/` directory at the project root.
    *   Key files will include:
        *   `versions.tf`: Defines Terraform and provider versions.
        *   `main.tf`: Contains the main resource definitions (VPC, EKS, RDS).
        *   `variables.tf`: Declares input variables for customization (e.g., region, instance types).
        *   `outputs.tf`: Defines output values (e.g., EKS cluster endpoint, RDS endpoint).
        *   `providers.tf`: Configures AWS provider.

2.  **Network Infrastructure (VPC)**:
    *   Provision a dedicated **Amazon VPC** for the EKS cluster and RDS database.
    *   Create public and private subnets across multiple Availability Zones for high availability.
    *   Set up an Internet Gateway for public subnet access and NAT Gateways for private subnet outbound internet access.

3.  **EKS Cluster Provisioning**:
    *   Define and create an **Amazon EKS Cluster**.
    *   Configure **IAM Roles** for the EKS control plane and worker nodes with necessary permissions.
    *   Provision an **EKS Node Group** (EC2 instances) that will serve as the worker nodes for our Kubernetes cluster. We will use a suitable instance type (e.g., `t3.medium` or `t3.large` for a small cluster).

4.  **RDS Database Provisioning**:
    *   Provision an **Amazon RDS PostgreSQL instance** (version 14).
    *   Place the RDS instance in private subnets within the VPC for enhanced security.
    *   Configure **Security Groups** to allow traffic only from the EKS cluster's security group to the RDS instance on the PostgreSQL port (5432).
    *   Define database name, username, and password.

5.  **Secrets Management**:
    *   For this training project, initial database credentials (username/password) will be passed directly as Terraform variables for simplicity.
    *   **Best Practice for Production**: In a production environment, these secrets should be managed by a dedicated service like AWS Secrets Manager and retrieved by the application at runtime.

6.  **Integration with ECR Image**:
    *   The Kubernetes deployment manifests (which we will create in Phase 4) will reference the Docker image that was built and pushed to your ECR repository in Phase 2. Terraform will provision the infrastructure, and Kubernetes will deploy the application using that image.

**Your Role (User) in this Phase:**
*   I will generate the Terraform code for each component.
*   You will be responsible for executing the Terraform commands locally:
    *   `terraform init`: Initializes the Terraform working directory.
    *   `terraform plan`: Shows what changes Terraform will make to your infrastructure.
    *   `terraform apply`: Executes the planned changes to provision the resources in your AWS account.
*   You will need to ensure your AWS CLI is configured with credentials that have the necessary permissions.

---

### **Phase 4: Continuous Deployment (CD)**

*Goal: Automate the release of the containerized application to the Kubernetes cluster.*

**Next Step: Implement this phase after successful Terraform apply.**

1.  **Create CD Workflow**: In GitHub Actions, create a new workflow that triggers after the CI workflow successfully builds and pushes an image from the `main` branch.
2.  **Manage Manifests with Kustomize**: The `k8s/` directory contains base manifests. Use **Kustomize** to manage environment-specific configurations without duplicating files. For example, Kustomize can be used to patch the `deployment.yml` with the correct image tag for the new release.
3.  **Automate Deployment**: The CD workflow will use `kubectl` to apply the Kustomize-generated manifests to the Kubernetes cluster.
    ```bash
    # Authenticate with the cluster
    # ...
    # Apply the manifests
    kustomize build overlays/production | kubectl apply -f -
    ```
4.  **Ensure Zero-Downtime Updates**: Configure the Kubernetes `Deployment` object with a `RollingUpdate` strategy and define `readinessProbe` and `livenessProbe` endpoints (leveraging Spring Boot Actuator's `/health` endpoints). This ensures the old version is not terminated until the new version is ready to serve traffic.

---

### **Phase 5: Monitoring & Observability**

*Goal: Gain deep insight into the application's performance and health in production.*

1.  **Metrics Collection**: Deploy **Prometheus** into the cluster to automatically scrape metrics from the Spring Boot Actuator's `/prometheus` endpoint.
2.  **Visualization**: Deploy **Grafana** and configure it with Prometheus as a data source. Create dashboards to visualize key application metrics (e.g., JVM performance, HTTP request latency, error rates).
3.  **Log Aggregation**: Set up a cluster-wide logging stack, such as **Fluentd**, **Elasticsearch**, and **Kibana (EFK)**, to collect, store, and search application logs from all running pods.
4.  **Alerting**: Configure **Alertmanager** (part of the Prometheus ecosystem) to send alerts to email, Slack, or PagerDuty when critical thresholds are breached (e.g., high CPU usage, low disk space, application error spikes).
